# CSAM Detector ğŸ”ğŸ›¡ï¸

A lightweight AI-powered prototype for detecting potential Child Sexual Abuse Material (CSAM) in images, with perceptual hashing, encrypted logging, and a secure web interface.

## ğŸš€ Features

- âœ… NSFW image detection using ResNet-50 (Yahoo Open NSFW model)
- ğŸ” Perceptual hash generation and matching (pHash)
- ğŸ§  Fast classification via OpenCV's DNN module
- ğŸ§¾ AES-256 encrypted SQLite logging
- ğŸ” Blurred image preview for analyst safety
- ğŸ“¦ Ready for local testing and educational use

## ğŸ§° Tech Stack

- Python 3.10+
- Flask (Web Framework)
- OpenCV (`cv2.dnn`)
- Pillow + ImageHash
- Cryptography (Fernet AES encryption)

## ğŸ“ Project Structure

```
csam-detector/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ routes.py, ai_model.py, hashing.py, database.py
â”œâ”€â”€ static/
â”œâ”€â”€ templates/
â”œâ”€â”€ model/               # Contains .caffemodel and prototxt
â”œâ”€â”€ hash_db/             # Simulated hash list
â”œâ”€â”€ secure_logs/         # Encrypted logs
â”œâ”€â”€ uploads/             # Uploaded images (blurred view only)
â””â”€â”€ run.py
```

## âš™ï¸ Getting Started

```bash
# Install dependencies
pip install -r requirements.txt

# Run the application
python run.py
```

Then open your browser at:  
`http://127.0.0.1:5000`

## âš ï¸ Ethical Use

This tool is intended **strictly for research, educational, and forensic training purposes**. It **does not store any real CSAM** and uses only public NSFW datasets and simulated hashes.

Please use responsibly and in compliance with local laws.

## ğŸ“š Related Work

- [Yahoo Open NSFW Model](https://github.com/yahoo/open_nsfw)
- [NuDetective (Brazilian Forensic Tool)](https://blog.ipog.edu.br/tecnologia/nudetective-ferramenta-pedofilia/)
- [CSAM Detection via Perceptual Hashing](https://www.microsoft.com/en-us/photodna)

## ğŸ”— Author

Developed by [Joas Antonio dos Santos](https://github.com/CyberSecurityUP)  
Repo: [https://github.com/CyberSecurityUP/csam-detector](https://github.com/CyberSecurityUP/csam-detector)
